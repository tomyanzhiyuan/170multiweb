<!-- School email: zyan17@u.rochester.edu -->

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 2 - LiveBench: AI Model Performance Benchmark</title>
    <link rel="stylesheet" href="css/styles.css">
</head>

<body>
    <header>
        <div class="container">
            <h1>LiveBench: A Comprehensive Benchmark for Large Language Models</h1>
        </div>
    </header>

    <main class="container">
        <section>
            <h2>Introduction</h2>
            <p>In the rapidly evolving field of artificial intelligence (AI), the development and assessment of Large
                Language Models (LLMs) have become paramount. Ensuring these models perform accurately and efficiently
                across diverse tasks necessitates robust benchmarking tools. One such tool that has emerged to address
                this need is LiveBench, a dynamic and contamination-free benchmark designed to evaluate LLMs
                comprehensively.</p>
        </section>

        <section>
            <h2>The Genesis of LiveBench</h2>
            <p>Traditional benchmarks for LLMs often face challenges related to test set contamination, where models
                inadvertently train on data that overlaps with evaluation sets. This overlap can lead to inflated
                performance metrics, providing a skewed understanding of a model's true capabilities. Recognizing this
                issue, a collaborative team from Abacus.AI, New York University, Nvidia, the University of Maryland, and
                the University of Southern California introduced LiveBench in mid-2024. This initiative aimed to create
                a benchmark resistant to contamination and capable of providing objective evaluations across a spectrum
                of tasks.</p>
        </section>

        <section>
            <h2>Key Features of LiveBench</h2>

            <article>
                <h3>Contamination-Free Test Data</h3>
                <p>LiveBench addresses the contamination issue by regularly updating its question sets. By sourcing
                    questions from recently released datasets, arXiv papers, news articles, and other current
                    information, it ensures that the evaluation data remains fresh and unlikely to have been seen by
                    models during training. This approach maintains the integrity of the benchmark and provides a
                    genuine assessment of a model's performance.</p>
            </article>

            <div class="grid-section">
                <article>
                    <h3>Diverse Task Categories</h3>
                    <p>The benchmark encompasses a wide array of tasks to evaluate various aspects of LLM capabilities.
                        These categories include:</p>
                    <ul>
                        <li><strong>Reasoning</strong>: Assessing logical deduction and problem-solving skills.</li>
                        <li><strong>Data Analysis</strong>: Evaluating the ability to interpret and analyze data sets.
                        </li>
                        <li><strong>Mathematics</strong>: Testing proficiency in mathematical computations and concepts.
                        </li>
                        <li><strong>Coding</strong>: Measuring code generation and understanding across different
                            programming languages.</li>
                        <li><strong>Language Comprehension</strong>: Evaluating understanding and processing of natural
                            language.</li>
                        <li><strong>Instruction Following</strong>: Testing adherence to given directives and tasks.
                        </li>
                    </ul>
                    <p>This diversity ensures a holistic evaluation of LLMs, highlighting strengths and identifying
                        areas
                        for improvement.</p>
                </article>

                <article>
                    <h3>Objective and Automated Scoring</h3>
                    <p>To eliminate biases associated with human evaluators or other AI judges, LiveBench employs
                        objective
                        ground-truth answers for its questions. This methodology allows for accurate and automatic
                        scoring,
                        even for complex tasks, ensuring consistency and reliability in evaluations.</p>

                    <h3>Regular Updates and Expansion</h3>
                    <p>Understanding the fast-paced nature of AI development, LiveBench commits to monthly updates of
                        its
                        question sets. This continuous infusion of new and challenging tasks ensures that the benchmark
                        remains relevant and capable of distinguishing between models as they evolve. Additionally, the
                        platform plans to introduce harder versions of existing tasks over time, pushing the boundaries
                        of
                        LLM capabilities.</p>
                </article>
            </div>
        </section>

        <section>
            <h2>Detailed Examination of Task Categories in LiveBench</h2>

            <article class="article-grid">
                <div>
                    <h3>Reasoning Tasks</h3>
                    <p>The Reasoning category is designed to assess the logical deduction and problem-solving skills of
                        LLMs. One of the prominent tasks in this category is an advanced version of the "Web of Lies"
                        from
                        the Big-Bench Hard benchmark. This task challenges models to navigate through complex networks
                        of
                        statements to determine their veracity, testing their ability to handle intricate logical
                        relationships.</p>
                    <p>Another task, known as the "Zebra Puzzles," presents models with logic grid puzzles that require
                        deducing relationships between different entities based on a set of constraints. These puzzles
                        are
                        renowned for their complexity and serve as a robust test of a model's reasoning capabilities.
                    </p>
                </div>
                <img src="images/ReasoningGraph.png"
                    alt="Bar chart showing top reasoning models led by OpenAI and Anthropic">
            </article>

            <article class="article-grid">
                <img src="images/DataGraph.png"
                    alt="Bar chart showing top data analysis models with diverse organizational representation">
                <div>
                    <h3>Data Analysis Tasks</h3>
                    <p>The Data Analysis category focuses on evaluating an LLM's ability to interpret and manipulate
                        data.
                        Tasks in this category are constructed using recent datasets from platforms like Kaggle and
                        Socrata,
                        ensuring that the data is both relevant and challenging. One such task involves table
                        reformatting,
                        where models are required to convert data between various formats, including JSON, CSV, TSV,
                        Markdown, and HTML. This assesses the model's versatility in handling different data
                        representations.</p>
                    <p>Another task requires models to predict which columns can be used to join two tables, testing
                        their
                        understanding of relational data structures. Additionally, models are tasked with determining
                        the
                        correct type annotations for data columns, evaluating their ability to infer data types based on
                        content.</p>
                </div>
            </article>

            <div class="grid-section">
                <article>
                    <h3>Mathematics Tasks</h3>
                    <p>The Mathematics category in LiveBench is crafted to test the mathematical reasoning and
                        problem-solving abilities of LLMs. This category includes questions sourced from recent high
                        school
                        mathematics competitions such as the American Mathematics Competitions (AMC12), the American
                        Invitational Mathematics Examination (AIME), the United States of America Mathematical Olympiad
                        (USAMO), the International Mathematical Olympiad (IMO), and the Senior Mathematical Challenge
                        (SMC).
                        These competitions present problems that require a deep understanding of various mathematical
                        domains, including algebra, geometry, number theory, and probability.</p>
                    <p>In addition to competition problems, LiveBench introduces the AMPS_Hard task, featuring
                        synthetically
                        generated questions inspired by the Advanced Mathematics Problem Solving (AMPS) dataset. These
                        questions are designed to push the boundaries of LLMs' mathematical capabilities, presenting
                        problems that require innovative approaches and advanced reasoning skills.</p>
                </article>

                <article>
                    <h3>Coding Tasks</h3>
                    <p>The Coding category evaluates an LLM's ability to understand and generate code across various
                        programming languages. Tasks in this category are derived from platforms like LeetCode and
                        AtCoder,
                        known for their challenging coding problems. LiveBench incorporates code generation questions
                        that
                        require models to produce correct and efficient code solutions based on given problem
                        statements.
                        Additionally, a novel code completion task is included, assessing the model's proficiency in
                        understanding incomplete code snippets and predicting the subsequent code accurately.</p>
                    <p>These tasks are crucial for determining how well LLMs can assist in real-world programming
                        scenarios,
                        from writing new code to debugging and completing existing codebases.</p>
                </article>
            </div>

            <div class="grid-section">
                <article>
                    <h3>Language Comprehension Tasks</h3>
                    <p>The Language Comprehension category assesses an LLM's proficiency in understanding and
                        manipulating
                        language. One of the tasks, inspired by the "Connections" word puzzle popularized by The New
                        York
                        Times, challenges models to group words based on hidden relationships, testing their semantic
                        understanding and lexical knowledge.</p>
                    <p>Another task, termed "Typos," involves presenting models with text containing intentional
                        misspellings. The model's objective is to correct these misspellings without altering the
                        original
                        style or meaning of the text. This task evaluates the model's attention to detail and its
                        ability to
                        maintain the integrity of the original content while making necessary corrections.</p>
                    <p>Additionally, the "Movie Scramble" task presents models with scrambled plots of recent movies,
                        requiring them to reorder the sequences coherently. This assesses the model's narrative
                        understanding and ability to comprehend temporal sequences in storytelling.</p>
                </article>

                <article>
                    <h3>Instruction Following Tasks</h3>
                    <p>The Instruction Following category examines how well LLMs can adhere to complex directives. Tasks
                        in
                        this category are based on recent articles from reputable sources like The Guardian. Models are
                        provided with specific instructions to perform tasks such as paraphrasing, simplifying,
                        summarizing,
                        or generating stories based on the content of these articles. Each task comes with multiple
                        randomly
                        drawn instructions, and the models are evaluated on their ability to follow these instructions
                        accurately and coherently.</p>
                    <p>This category is essential for assessing the practical utility of LLMs in applications where
                        following user instructions precisely is critical, such as virtual assistants and content
                        generation
                        tools.</p>
                </article>
            </div>
        </section>

        <section>
            <h2>AI Model Performance Comparison</h2>
            <p>The table below presents a comprehensive comparison of various AI models' performance across different
                categories as measured by LiveBench. The data shows how models from leading organizations like
                Anthropic, OpenAI, Google, and others perform on reasoning, coding, mathematics, data analysis, and
                language tasks.</p>

            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Organization</th>
                        <th>Global Average</th>
                        <th>Reasoning Average</th>
                        <th>Coding Average</th>
                        <th>Mathematics Average</th>
                        <th>Data Analysis Average</th>
                        <th>Language Average</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>claude-3-7-sonnet-thinking</td>
                        <td>Anthropic</td>
                        <td>75.07</td>
                        <td>87.83</td>
                        <td>74.54</td>
                        <td>79.00</td>
                        <td>74.05</td>
                        <td>59.93</td>
                    </tr>
                    <tr>
                        <td>o1-2024-12-17-high</td>
                        <td>OpenAI</td>
                        <td>74.49</td>
                        <td>91.58</td>
                        <td>69.69</td>
                        <td>80.32</td>
                        <td>65.47</td>
                        <td>65.39</td>
                    </tr>
                    <tr>
                        <td>o3-mini-2025-01-31-high</td>
                        <td>OpenAI</td>
                        <td>74.19</td>
                        <td>89.58</td>
                        <td>82.74</td>
                        <td>77.29</td>
                        <td>70.64</td>
                        <td>50.68</td>
                    </tr>
                    <tr>
                        <td>deepseek-r1</td>
                        <td>DeepSeek</td>
                        <td>69.79</td>
                        <td>83.17</td>
                        <td>66.74</td>
                        <td>80.71</td>
                        <td>69.78</td>
                        <td>48.53</td>
                    </tr>
                    <tr>
                        <td>o3-mini-2025-01-31-medium</td>
                        <td>OpenAI</td>
                        <td>67.38</td>
                        <td>86.33</td>
                        <td>65.38</td>
                        <td>72.37</td>
                        <td>66.56</td>
                        <td>46.26</td>
                    </tr>
                    <tr>
                        <td>gemini-2.0-flash-thinking-exp-01-21</td>
                        <td>Google</td>
                        <td>63.81</td>
                        <td>78.17</td>
                        <td>53.49</td>
                        <td>75.85</td>
                        <td>69.37</td>
                        <td>42.18</td>
                    </tr>
                    <tr>
                        <td>claude-3-7-sonnet</td>
                        <td>Anthropic</td>
                        <td>63.38</td>
                        <td>86.00</td>
                        <td>67.49</td>
                        <td>63.26</td>
                        <td>63.37</td>
                        <td>56.76</td>
                    </tr>
                    <tr>
                        <td>gemini-2.0-pro-exp-02-05</td>
                        <td>Google</td>
                        <td>61.48</td>
                        <td>60.08</td>
                        <td>63.49</td>
                        <td>70.97</td>
                        <td>68.02</td>
                        <td>44.85</td>
                    </tr>
                    <tr>
                        <td>gemini-exp-1206</td>
                        <td>Google</td>
                        <td>61.44</td>
                        <td>57.00</td>
                        <td>63.41</td>
                        <td>72.36</td>
                        <td>63.16</td>
                        <td>51.29</td>
                    </tr>
                    <tr>
                        <td>qwen2.5-max</td>
                        <td>Alibaba</td>
                        <td>59.68</td>
                        <td>51.42</td>
                        <td>64.41</td>
                        <td>58.35</td>
                        <td>67.93</td>
                        <td>56.28</td>
                    </tr>
                    <tr>
                        <td>o3-mini-2025-01-31-low</td>
                        <td>OpenAI</td>
                        <td>58.93</td>
                        <td>69.83</td>
                        <td>61.46</td>
                        <td>63.06</td>
                        <td>62.04</td>
                        <td>38.25</td>
                    </tr>
                    <tr>
                        <td>deepseek-v3</td>
                        <td>DeepSeek</td>
                        <td>57.50</td>
                        <td>56.75</td>
                        <td>61.77</td>
                        <td>60.54</td>
                        <td>60.94</td>
                        <td>47.48</td>
                    </tr>
                    <tr>
                        <td>claude-3-5-sonnet-20241022</td>
                        <td>Anthropic</td>
                        <td>56.97</td>
                        <td>56.67</td>
                        <td>67.13</td>
                        <td>52.28</td>
                        <td>55.03</td>
                        <td>53.76</td>
                    </tr>
                    <tr>
                        <td>gemini-2.0-flash</td>
                        <td>Google</td>
                        <td>56.60</td>
                        <td>55.25</td>
                        <td>53.92</td>
                        <td>65.62</td>
                        <td>67.55</td>
                        <td>40.69</td>
                    </tr>
                    <tr>
                        <td>chatgpt-4o-latest-2025-01-29</td>
                        <td>OpenAI</td>
                        <td>56.33</td>
                        <td>57.92</td>
                        <td>60.56</td>
                        <td>48.02</td>
                        <td>66.00</td>
                        <td>49.14</td>
                    </tr>
                    <tr>
                        <td>o1-mini-2024-09-12</td>
                        <td>OpenAI</td>
                        <td>56.24</td>
                        <td>72.33</td>
                        <td>48.05</td>
                        <td>61.99</td>
                        <td>57.92</td>
                        <td>40.89</td>
                    </tr>
                    <tr>
                        <td>gemini-2.0-flash-exp</td>
                        <td>Google</td>
                        <td>54.74</td>
                        <td>59.08</td>
                        <td>54.36</td>
                        <td>60.39</td>
                        <td>61.67</td>
                        <td>38.22</td>
                    </tr>
                    <tr>
                        <td>gpt-4o-2024-08-06</td>
                        <td>OpenAI</td>
                        <td>52.68</td>
                        <td>53.92</td>
                        <td>51.44</td>
                        <td>49.54</td>
                        <td>60.91</td>
                        <td>47.59</td>
                    </tr>
                    <tr>
                        <td>deepseek-r1-distill-llama-70b</td>
                        <td>DeepSeek</td>
                        <td>51.41</td>
                        <td>67.58</td>
                        <td>51.62</td>
                        <td>58.11</td>
                        <td>55.93</td>
                        <td>23.81</td>
                    </tr>
                    <tr>
                        <td>step-2-16k-202411</td>
                        <td>StepFun</td>
                        <td>51.25</td>
                        <td>52.17</td>
                        <td>47.19</td>
                        <td>48.77</td>
                        <td>63.72</td>
                        <td>44.39</td>
                    </tr>
                    <tr>
                        <td>grok-2-1212</td>
                        <td>xAI</td>
                        <td>51.24</td>
                        <td>54.83</td>
                        <td>46.44</td>
                        <td>54.88</td>
                        <td>54.45</td>
                        <td>45.58</td>
                    </tr>
                    <tr>
                        <td>gemini-1.5-pro-002</td>
                        <td>Google</td>
                        <td>51.04</td>
                        <td>49.08</td>
                        <td>48.80</td>
                        <td>59.07</td>
                        <td>54.97</td>
                        <td>43.29</td>
                    </tr>
                    <tr>
                        <td>dracarys2-72b-instruct</td>
                        <td>AbacusAI</td>
                        <td>50.12</td>
                        <td>47.38</td>
                        <td>58.92</td>
                        <td>54.66</td>
                        <td>55.51</td>
                        <td>34.12</td>
                    </tr>
                    <tr>
                        <td>gpt-4o-2024-11-20</td>
                        <td>OpenAI</td>
                        <td>49.64</td>
                        <td>55.75</td>
                        <td>46.08</td>
                        <td>42.87</td>
                        <td>56.15</td>
                        <td>47.37</td>
                    </tr>
                    <tr>
                        <td>gemini-2.0-flash-lite-preview-02-05</td>
                        <td>Google</td>
                        <td>48.23</td>
                        <td>50.08</td>
                        <td>43.80</td>
                        <td>55.54</td>
                        <td>57.47</td>
                        <td>34.28</td>
                    </tr>
                    <tr>
                        <td>meta-llama-3.1-405b-instruct-turbo</td>
                        <td>Meta</td>
                        <td>47.66</td>
                        <td>53.25</td>
                        <td>42.65</td>
                        <td>41.07</td>
                        <td>55.85</td>
                        <td>45.46</td>
                    </tr>
                    <tr>
                        <td>claude-3-opus-20240229</td>
                        <td>Anthropic</td>
                        <td>46.21</td>
                        <td>40.58</td>
                        <td>38.59</td>
                        <td>43.62</td>
                        <td>57.89</td>
                        <td>50.39</td>
                    </tr>
                    <tr>
                        <td>grok-beta</td>
                        <td>xAI</td>
                        <td>45.09</td>
                        <td>37.00</td>
                        <td>45.15</td>
                        <td>45.84</td>
                        <td>54.27</td>
                        <td>43.16</td>
                    </tr>
                    <tr>
                        <td>mistral-large-2411</td>
                        <td>Mistral AI</td>
                        <td>44.53</td>
                        <td>43.50</td>
                        <td>47.08</td>
                        <td>42.55</td>
                        <td>50.15</td>
                        <td>39.39</td>
                    </tr>
                    <tr>
                        <td>qwen2.5-coder-32b-instruct</td>
                        <td>Alibaba</td>
                        <td>43.73</td>
                        <td>42.08</td>
                        <td>56.85</td>
                        <td>46.61</td>
                        <td>49.87</td>
                        <td>23.25</td>
                    </tr>
                    <tr>
                        <td>llama-3.3-70b-instruct-turbo</td>
                        <td>Meta</td>
                        <td>43.65</td>
                        <td>50.75</td>
                        <td>36.59</td>
                        <td>42.24</td>
                        <td>49.49</td>
                        <td>39.20</td>
                    </tr>
                    <tr>
                        <td>deepseek-r1-distill-qwen-32b</td>
                        <td>DeepSeek</td>
                        <td>43.52</td>
                        <td>52.25</td>
                        <td>33.67</td>
                        <td>59.44</td>
                        <td>45.41</td>
                        <td>26.82</td>
                    </tr>
                    <tr>
                        <td>dracarys2-llama-3.1-70b-instruct</td>
                        <td>AbacusAI</td>
                        <td>42.81</td>
                        <td>44.67</td>
                        <td>36.31</td>
                        <td>40.30</td>
                        <td>53.98</td>
                        <td>38.78</td>
                    </tr>
                    <tr>
                        <td>qwq-32b-preview</td>
                        <td>Alibaba</td>
                        <td>41.18</td>
                        <td>57.71</td>
                        <td>37.20</td>
                        <td>58.26</td>
                        <td>31.62</td>
                        <td>21.09</td>
                    </tr>
                    <tr>
                        <td>meta-llama-3.1-70b-instruct-turbo</td>
                        <td>Meta</td>
                        <td>40.08</td>
                        <td>43.00</td>
                        <td>33.49</td>
                        <td>34.72</td>
                        <td>53.75</td>
                        <td>35.42</td>
                    </tr>
                    <tr>
                        <td>claude-3-5-haiku-20241022</td>
                        <td>Anthropic</td>
                        <td>39.76</td>
                        <td>28.08</td>
                        <td>51.36</td>
                        <td>35.54</td>
                        <td>48.45</td>
                        <td>35.37</td>
                    </tr>
                    <tr>
                        <td>mistral-small-2501</td>
                        <td>Mistral AI</td>
                        <td>39.15</td>
                        <td>36.42</td>
                        <td>35.31</td>
                        <td>39.89</td>
                        <td>53.69</td>
                        <td>30.46</td>
                    </tr>
                    <tr>
                        <td>amazon-nova-pro-v1:0</td>
                        <td>Amazon</td>
                        <td>38.81</td>
                        <td>32.58</td>
                        <td>38.15</td>
                        <td>38.04</td>
                        <td>48.31</td>
                        <td>36.96</td>
                    </tr>
                    <tr>
                        <td>phi-4</td>
                        <td>Microsoft</td>
                        <td>38.25</td>
                        <td>47.83</td>
                        <td>30.67</td>
                        <td>41.98</td>
                        <td>45.17</td>
                        <td>25.61</td>
                    </tr>
                    <tr>
                        <td>gpt-4o-mini-2024-07-18</td>
                        <td>OpenAI</td>
                        <td>38.16</td>
                        <td>32.75</td>
                        <td>43.15</td>
                        <td>36.31</td>
                        <td>49.96</td>
                        <td>28.61</td>
                    </tr>
                    <tr>
                        <td>gamma-2-27b-it</td>
                        <td>Google</td>
                        <td>34.20</td>
                        <td>28.08</td>
                        <td>35.95</td>
                        <td>26.46</td>
                        <td>47.87</td>
                        <td>32.62</td>
                    </tr>
                    <tr>
                        <td>amazon-nova-lite-v1:0</td>
                        <td>Amazon</td>
                        <td>32.80</td>
                        <td>36.67</td>
                        <td>27.46</td>
                        <td>36.70</td>
                        <td>37.23</td>
                        <td>25.93</td>
                    </tr>
                    <tr>
                        <td>qwen2.5-7b-instruct-turbo</td>
                        <td>Alibaba</td>
                        <td>31.46</td>
                        <td>28.42</td>
                        <td>38.37</td>
                        <td>39.51</td>
                        <td>35.22</td>
                        <td>15.80</td>
                    </tr>
                    <tr>
                        <td>mistral-small-2409</td>
                        <td>Mistral AI</td>
                        <td>29.46</td>
                        <td>29.92</td>
                        <td>25.74</td>
                        <td>24.42</td>
                        <td>42.73</td>
                        <td>24.49</td>
                    </tr>
                    <tr>
                        <td>command-r-plus-08-2024</td>
                        <td>Cohere</td>
                        <td>26.59</td>
                        <td>24.75</td>
                        <td>19.14</td>
                        <td>21.27</td>
                        <td>38.06</td>
                        <td>29.73</td>
                    </tr>
                    <tr>
                        <td>amazon-nova-micro-v1:0</td>
                        <td>Amazon</td>
                        <td>25.90</td>
                        <td>25.08</td>
                        <td>20.18</td>
                        <td>34.49</td>
                        <td>33.95</td>
                        <td>15.78</td>
                    </tr>
                    <tr>
                        <td>gemma-2-9b-it</td>
                        <td>Google</td>
                        <td>23.87</td>
                        <td>15.17</td>
                        <td>22.46</td>
                        <td>19.80</td>
                        <td>36.39</td>
                        <td>25.53</td>
                    </tr>
                    <tr>
                        <td>command-r-08-2024</td>
                        <td>Cohere</td>
                        <td>21.85</td>
                        <td>21.92</td>
                        <td>17.90</td>
                        <td>19.39</td>
                        <td>33.34</td>
                        <td>16.72</td>
                    </tr>
                    <tr>
                        <td>command-r-plus-04-2024</td>
                        <td>Cohere</td>
                        <td>20.64</td>
                        <td>20.58</td>
                        <td>19.46</td>
                        <td>17.99</td>
                        <td>25.48</td>
                        <td>19.70</td>
                    </tr>
                    <tr>
                        <td>meta-llama-3.1-8b-instruct-turbo</td>
                        <td>Meta</td>
                        <td>20.18</td>
                        <td>13.33</td>
                        <td>18.74</td>
                        <td>18.31</td>
                        <td>32.82</td>
                        <td>17.71</td>
                    </tr>
                    <tr>
                        <td>phi-3-small-8k-instruct</td>
                        <td>Microsoft</td>
                        <td>19.40</td>
                        <td>15.92</td>
                        <td>20.26</td>
                        <td>17.58</td>
                        <td>30.29</td>
                        <td>12.94</td>
                    </tr>
                    <tr>
                        <td>phi-3-mini-4k-instruct</td>
                        <td>Microsoft</td>
                        <td>19.22</td>
                        <td>26.83</td>
                        <td>15.54</td>
                        <td>14.96</td>
                        <td>30.21</td>
                        <td>8.56</td>
                    </tr>
                    <tr>
                        <td>phi-3-mini-128k-instruct</td>
                        <td>Microsoft</td>
                        <td>19.02</td>
                        <td>20.50</td>
                        <td>15.04</td>
                        <td>15.72</td>
                        <td>34.69</td>
                        <td>9.15</td>
                    </tr>
                    <tr>
                        <td>olmo-2-1124-13b-instruct</td>
                        <td>AllenAI</td>
                        <td>14.43</td>
                        <td>16.33</td>
                        <td>10.41</td>
                        <td>13.64</td>
                        <td>20.60</td>
                        <td>11.16</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>Top Performing AI Models by Category</h2>
            <p>Based on the LiveBench data, we can identify the top-performing models in each evaluation category:</p>

            <aside>
                <h3>Performance Highlights</h3>
                <p>The data reveals interesting patterns in how different organizations optimize their models. While
                    some focus on specific capabilities like reasoning or coding, others aim for balanced performance
                    across all categories.</p>
                <p>Notably, models with "thinking" capabilities (like claude-3-7-sonnet-thinking) tend to perform
                    significantly better than their base counterparts, highlighting the importance of deliberative
                    processes in complex tasks.</p>
            </aside>

            <div class="grid-section">
                <article>
                    <h3>Reasoning Champions</h3>
                    <img src="images/ReasoningGraph.png"
                        alt="Bar chart showing top reasoning models led by OpenAI and Anthropic">
                    <ul>
                        <li><strong>o1-2024-12-17-high (OpenAI)</strong>: 91.58%</li>
                        <li><strong>o3-mini-2025-01-31-high (OpenAI)</strong>: 89.58%</li>
                        <li><strong>claude-3-7-sonnet-thinking (Anthropic)</strong>: 87.83%</li>
                        <li><strong>o3-mini-2025-01-31-medium (OpenAI)</strong>: 86.33%</li>
                        <li><strong>claude-3-7-sonnet (Anthropic)</strong>: 86.00%</li>
                    </ul>
                    <p>OpenAI's models clearly dominate in reasoning tasks, with their o1 and o3 models taking the top
                        spots. Anthropic's Claude models also demonstrate exceptional capabilities in this category,
                        showing
                        the importance of advanced reasoning capabilities in state-of-the-art LLMs.</p>
                </article>

                <article>
                    <h3>Coding Specialists</h3>
                    <img src="images/CodingGraph.png"
                        alt="Bar chart showing top coding models with OpenAI's o3-mini leading significantly">
                    <ul>
                        <li><strong>o3-mini-2025-01-31-high (OpenAI)</strong>: 82.74%</li>
                        <li><strong>claude-3-7-sonnet-thinking (Anthropic)</strong>: 74.54%</li>
                        <li><strong>o1-2024-12-17-high (OpenAI)</strong>: 69.69%</li>
                        <li><strong>claude-3-7-sonnet (Anthropic)</strong>: 67.49%</li>
                        <li><strong>claude-3-5-sonnet-20241022 (Anthropic)</strong>: 67.13%</li>
                    </ul>
                    <p>OpenAI's o3-mini-2025-01-31-high model significantly outperforms all others in coding tasks, with
                        an
                        impressive 8-point lead over the next best model. Anthropic's models show strong coding
                        capabilities
                        as well, with multiple Claude variants appearing in the top performers.</p>
                </article>
            </div>

            <div class="grid-section">
                <article>
                    <h3>Mathematics Performers</h3>
                    <img src="images/MathGraph.png"
                        alt="Bar chart showing top mathematical reasoning models with diverse organizational representation">
                    <ul>
                        <li><strong>deepseek-r1 (DeepSeek)</strong>: 80.71%</li>
                        <li><strong>o1-2024-12-17-high (OpenAI)</strong>: 80.32%</li>
                        <li><strong>claude-3-7-sonnet-thinking (Anthropic)</strong>: 79.00%</li>
                        <li><strong>o3-mini-2025-01-31-high (OpenAI)</strong>: 77.29%</li>
                        <li><strong>gemini-2.0-flash-thinking-exp-01-21 (Google)</strong>: 75.85%</li>
                    </ul>
                    <p>Mathematical reasoning shows more diversity among top performers, with DeepSeek's r1 model
                        narrowly
                        edging out OpenAI's o1 model. Google also makes an appearance in the top five with their Gemini
                        model, demonstrating that mathematical capabilities are more evenly distributed across
                        organizations
                        than other skills.</p>
                </article>

                <article>
                    <h3>Data Analysis Champions</h3>
                    <img src="images/DataGraph.png"
                        alt="Bar chart showing top data analysis models with diverse organizational representation">
                    <ul>
                        <li><strong>claude-3-7-sonnet-thinking (Anthropic)</strong>: 74.05%</li>
                        <li><strong>deepseek-r1 (DeepSeek)</strong>: 69.78%</li>
                        <li><strong>gemini-2.0-flash-thinking-exp-01-21 (Google)</strong>: 69.37%</li>
                        <li><strong>o3-mini-2025-01-31-high (OpenAI)</strong>: 70.64%</li>
                        <li><strong>qwen2.5-max (Alibaba)</strong>: 67.93%</li>
                    </ul>
                    <p>Data analysis capabilities show interesting diversity, with Anthropic's
                        claude-3-7-sonnet-thinking
                        taking the top spot. Notably, Alibaba's qwen2.5-max appears in the top five, demonstrating
                        competitive performance in this practical domain despite lower scores in other categories.</p>
                </article>
            </div>

            <article>
                <h3>Language Proficiency Leaders</h3>
                <img src="images/LanguageGraph.png"
                    alt="Bar chart showing top language models with OpenAI's o1 model in the lead">
                <ul>
                    <li><strong>o1-2024-12-17-high (OpenAI)</strong>: 65.39%</li>
                    <li><strong>claude-3-7-sonnet-thinking (Anthropic)</strong>: 59.93%</li>
                    <li><strong>claude-3-7-sonnet (Anthropic)</strong>: 56.76%</li>
                    <li><strong>qwen2.5-max (Alibaba)</strong>: 56.28%</li>
                    <li><strong>claude-3-5-sonnet-20241022 (Anthropic)</strong>: 53.76%</li>
                </ul>
                <p>Language tasks appear to be the most challenging category overall, with significantly lower scores
                    compared to other categories. OpenAI's o1 model leads by a substantial margin, while Anthropic shows
                    consistent strength with multiple models in the top five. Alibaba's qwen2.5-max demonstrates
                    impressive language capabilities, outperforming many models from larger organizations.</p>
            </article>
        </section>

        <section>
            <h2>Comparative Analysis with Other Benchmarks</h2>
            <p>In the landscape of AI evaluation, several benchmarks have been developed to assess the performance of
                Large Language Models. Comparing LiveBench with other prominent benchmarks reveals distinct approaches
                and highlights the unique contributions of each.</p>

            <div class="grid-section">
                <article>
                    <h3>Chatbot Arena</h3>
                    <p>Chatbot Arena, developed by researchers at UC Berkeley, offers a platform where users can compare
                        AI
                        chatbot responses in a head-to-head format. This benchmark emphasizes user preferences by
                        allowing
                        individuals to vote on which model's response they favor. While this approach captures human
                        subjective judgment, it may introduce biases and lacks the objective scoring mechanism that
                        LiveBench employs. Additionally, Chatbot Arena's reliance on user interactions means that its
                        evaluations are influenced by the participants' varying expertise and expectations.</p>
                </article>

                <article>
                    <h3>Arena-Hard</h3>
                    <p>Arena-Hard is another benchmark that focuses on evaluating LLMs through challenging tasks.
                        However,
                        it has been noted that models like GPT-4 perform substantially better on Arena-Hard compared to
                        LiveBench. This discrepancy is likely due to the known bias from using GPT-4 itself as the LLM
                        judge
                        in Arena-Hard, which can lead to inflated performance metrics. In contrast, LiveBench mitigates
                        such
                        biases by employing objective ground-truth answers and regularly updating its question sets to
                        prevent test set contamination.</p>
                </article>
            </div>

            <article>
                <h3>LiveXiv</h3>
                <p>IBM's LiveXiv is a dynamic benchmark designed to evaluate vision-language models on tasks involving
                    visual document understanding. Similar to LiveBench, LiveXiv addresses the issue of test set
                    contamination by updating its questions monthly, sourcing them from recent scientific publications.
                    This approach ensures that models are tested on unseen data, providing a more accurate assessment of
                    their capabilities. While LiveXiv focuses on multimodal tasks, its methodology aligns with
                    LiveBench's commitment to maintaining benchmark integrity through frequent updates and
                    contamination-free test sets.</p>
            </article>
        </section>

        <section>
            <h2>Future Directions and Developments</h2>
            <p>LiveBench is committed to evolving in tandem with advancements in Large Language Models (LLMs), ensuring
                that it remains a relevant and rigorous benchmark for AI evaluation. Several future directions and
                developments are planned to enhance its robustness and applicability.</p>

            <div class="grid-section">
                <article>
                    <h3>Planned Enhancements</h3>
                    <p><strong>Regular Updates with Increasing Complexity</strong>: To prevent test set contamination
                        and
                        maintain the benchmark's challenge level, LiveBench introduces new questions monthly, sourced
                        from
                        recent information such as newly released math competitions, arXiv papers, and datasets. This
                        approach ensures that the questions remain novel and progressively more challenging, effectively
                        distinguishing between the capabilities of emerging LLMs.</p>
                    <p><strong>Expansion of Task Diversity</strong>: Recognizing the multifaceted applications of LLMs,
                        LiveBench plans to broaden its spectrum of tasks. Future updates aim to include non-English
                        language
                        tasks, thereby evaluating models' multilingual proficiencies. Additionally, there is an emphasis
                        on
                        incorporating tasks that require multimodal understanding, such as interpreting visual data
                        alongside textual information, to mirror real-world scenarios more accurately.</p>
                </article>

                <article>
                    <h3>Community Involvement</h3>
                    <p><strong>Open-Source Collaboration</strong>: LiveBench operates under an open-source framework,
                        inviting researchers and developers worldwide to contribute. This collaborative approach
                        encourages
                        the addition of new questions, tasks, and models, fostering a dynamic and inclusive environment
                        that
                        accelerates the benchmark's evolution in line with the latest AI research trends.</p>
                    <p><strong>Transparent Evaluation Processes</strong>: By making all questions, code, and model
                        answers
                        publicly accessible, LiveBench ensures transparency in its evaluation processes. This openness
                        allows for collective scrutiny, feedback, and enhancement, building trust within the AI
                        community
                        and ensuring the benchmark's methodologies remain robust and unbiased.</p>
                </article>
            </div>
        </section>

        <section>
            <h2>Impact on AI Research and Development</h2>
            <p>LiveBench has significantly influenced the trajectory of AI research and development by introducing a
                dynamic and contamination-free benchmarking system for Large Language Models (LLMs). Its innovative
                approach addresses longstanding challenges in model evaluation, thereby fostering more accurate
                assessments and driving advancements in the field.</p>

            <div class="grid-section">
                <article>
                    <h3>Addressing Test Set Contamination</h3>
                    <p>A critical issue in AI model evaluation is test set contamination, where models inadvertently
                        train
                        on data that overlaps with benchmark test sets, leading to inflated performance metrics.
                        LiveBench
                        mitigates this problem by regularly updating its question sets with fresh content from recent
                        sources such as newly released math competitions, arXiv papers, and datasets. This strategy
                        ensures
                        that the evaluation data remains novel and unseen during model training, providing a more
                        authentic
                        measure of a model's capabilities.</p>
                </article>

                <article>
                    <h3>Enhancing Benchmark Diversity and Challenge</h3>
                    <p>By encompassing a wide array of tasks—including mathematics, coding, reasoning, language
                        comprehension, instruction following, and data analysis—LiveBench offers a comprehensive
                        evaluation
                        framework. This diversity compels researchers to develop models with broader skill sets and
                        adaptability, moving beyond narrow task-specific proficiencies. The inclusion of challenging,
                        contamination-free tasks encourages the creation of more robust and versatile AI systems.</p>
                </article>
            </div>

            <div class="grid-section">
                <article>
                    <h3>Promoting Objective and Transparent Evaluation</h3>
                    <p>LiveBench employs automatic scoring mechanisms based on objective ground-truth values, reducing
                        reliance on subjective human judgments that can introduce biases. This objectivity enhances the
                        credibility of evaluation outcomes. Moreover, by releasing all questions, code, and model
                        answers
                        publicly, LiveBench fosters transparency and reproducibility in AI research, enabling the
                        community
                        to validate and build upon existing work.</p>
                </article>

                <article>
                    <h3>Catalyzing Community Engagement and Collaboration</h3>
                    <p>The open-source nature of LiveBench invites researchers and developers worldwide to contribute by
                        adding new tasks, questions, and models. This collaborative environment accelerates the
                        evolution of
                        the benchmark, ensuring it remains aligned with the latest advancements and challenges in AI.
                        Such
                        community-driven development promotes the sharing of best practices and collective
                        problem-solving,
                        essential for the rapid progression of AI technologies.</p>
                </article>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <h2>References</h2>
            <ol>
                <li><a href="https://livebench.ai/#/">LiveBench Website</a></li>
                <li><a
                        href="https://venturebeat.com/ai/livebench-open-ai-model-benchmark-contamination-free-test-data/?utm_source">VentureBeat
                        Article</a></li>
                <li><a href="https://openreview.net/forum?id=sKYHBTAxVa&utm_source">Research Paper on OpenReview</a>
                </li>
                <li><a
                        href="https://www.wsj.com/tech/ai/the-uc-berkeley-project-that-is-the-ai-industrys-obsession-bc68b3e3?utm_source">The
                        UC Berkeley Project That Is the AI Industry's Obsession</a></li>
                <li><a href="https://github.com/LiveBench/LiveBench?utm_source">LiveBench's Github Repository</a></li>
                <li><a href="https://research.ibm.com/blog/live-VQA-benchmark?utm_source">IBM Research: LiveXiv - A
                        Dynamic
                        Benchmark for Multimodal AI</a></li>
            </ol>
            <p>
                <small>Created with energy by Tom Yan for CSC170 Project 02 ⚡ | Copyright &copy; 2025 Tom Yan</small>
            </p>
        </div>
    </footer>
</body>

</html>